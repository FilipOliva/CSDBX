{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9dd1a5-be5b-4b74-b477-04d0e8981c2d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760614773498}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Table Size Report - Plain Text for Excel\n",
    "\n",
    "# ============================================================================\n",
    "# TABLE LIST - Edit this section\n",
    "# ============================================================================\n",
    "table_list = [\n",
    "\"gap_catalog.ads_owner.case_phase_properties\",\n",
    "\"gap_catalog.ads_src_20250901.dlk_ads_sma_monitor_events_full\",\n",
    "\"gap_catalog.ads_owner.SMA_CASE_PHASE_PROPERTIES\",\n",
    "\"gap_catalog.ads_etl_owner.STG_SMA_CASE_PHASE_PROPERTIES_HUMANTASK\",\n",
    "\"gap_catalog.ads_etl_owner.STG_SMA_CASE_PHASE_PROPERTIES_PARSE\",\n",
    "\"gap_catalog.ads_etl_owner.STG_SMA_CASE_PHASE_PROPERTIES\",\n",
    "\"gap_catalog.ads_owner.CASES\",\n",
    "\"gap_catalog.ads_owner.CASE_PHASES\",\n",
    "\"gap_catalog.ads_owner.CASE_PHASE_PROPERTY_TYPES\",\n",
    "\"gap_catalog.ads_OWNER.case_types\",\n",
    "\"gap_catalog.ADS_ETL_OWNER.STG_SMA_PROCESS_EVENTS_PARSE\",\n",
    "\"gap_catalog.dwh_owner.parties\",\n",
    "\"gap_catalog.ads_src_20250901.event_types\",\n",
    "\"gap_catalog.ads_src_20250901.status_reason\",\n",
    "\"gap_catalog.ads_src_20250901.PROD_INST_CASES\",\n",
    "\"gap_catalog.ads_owner.PROCESS_EVENTS\",\n",
    "\"gap_catalog.ads_etl_owner.XC_SMA_CASE_PHASE_PROPERTIES_HUMANTASK\",\n",
    "\"gap_catalog.ads_etl_owner.XC_SMA_CASE_PHASE_PROPERTIES_MAIN\",\n",
    "\"gap_catalog.ads_etl_owner.XC_SMA_CASE_PHASE_PROPERTIES_POT_OWN\",\n",
    "\"gap_catalog.ads_etl_owner.XC_SMA_PROCESS_EVENTS_MAIN\",\n",
    "\"gap_catalog.ads_etl_owner.XC_STG_SMA_CASE_PHASE_PROPERTIES_MAIN\",\n",
    "\"gap_catalog.ads_etl_owner.XC_STG_SMA_CASE_PHASE_PROPERTIES_UPDATE\"\n",
    "]\n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Get Table Sizes and Row Counts\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Analyzing {len(table_list)} tables...\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, table_name in enumerate(table_list, 1):\n",
    "    try:\n",
    "        print(f\"[{i}/{len(table_list)}] Processing {table_name}...\", end=\" \")\n",
    "        \n",
    "        # Get table details\n",
    "        detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\").first()\n",
    "        \n",
    "        # Get row count\n",
    "        row_count = spark.table(table_name).count()\n",
    "        \n",
    "        # Split table name into schema and table\n",
    "        parts = table_name.split('.')\n",
    "        if len(parts) == 3:\n",
    "            catalog = parts[0]\n",
    "            schema = parts[1]\n",
    "            table = parts[2]\n",
    "            full_schema = f\"{catalog}.{schema}\"\n",
    "        elif len(parts) == 2:\n",
    "            schema = parts[0]\n",
    "            table = parts[1]\n",
    "            full_schema = schema\n",
    "        else:\n",
    "            full_schema = \"\"\n",
    "            table = table_name\n",
    "        \n",
    "        # Calculate size in GB\n",
    "        size_gb = round(detail.sizeInBytes / (1024**3), 2)\n",
    "        \n",
    "        results.append({\n",
    "            \"Schema\": full_schema,\n",
    "            \"Table\": table,\n",
    "            \"Storage_GB\": size_gb,\n",
    "            \"Row_count\": row_count,\n",
    "            \"type\": detail.format,\n",
    "            \"num_files\": detail.numFiles,\n",
    "            \"format\": detail.format\n",
    "        })\n",
    "        \n",
    "        print(f\"✅ {size_gb} GB, {row_count} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle errors\n",
    "        parts = table_name.split('.')\n",
    "        if len(parts) >= 2:\n",
    "            full_schema = '.'.join(parts[:-1])\n",
    "            table = parts[-1]\n",
    "        else:\n",
    "            full_schema = \"\"\n",
    "            table = table_name\n",
    "            \n",
    "        print(f\"❌ ERROR: {str(e)}\")\n",
    "        results.append({\n",
    "            \"Schema\": full_schema,\n",
    "            \"Table\": table,\n",
    "            \"Storage_GB\": 0,\n",
    "            \"Row_count\": 0,\n",
    "            \"type\": \"ERROR\",\n",
    "            \"num_files\": 0,\n",
    "            \"format\": \"ERROR\"\n",
    "        })\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"✅ Analysis complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Display Results - Excel Ready Format\n",
    "\n",
    "# Create DataFrame with specific column order\n",
    "result_df = spark.createDataFrame(results)\n",
    "\n",
    "# Reorder columns\n",
    "result_df = result_df.select(\n",
    "    \"Schema\",\n",
    "    \"Table\", \n",
    "    \"Storage_GB\",\n",
    "    \"Row_count\",\n",
    "    \"type\",\n",
    "    \"num_files\",\n",
    "    \"format\"\n",
    ")\n",
    "\n",
    "# Sort by Storage_GB descending\n",
    "result_df_sorted = result_df.orderBy(col(\"Storage_GB\").desc())\n",
    "\n",
    "# Display\n",
    "display(result_df_sorted)\n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1, Print Summary\n",
    "\n",
    "from pyspark.sql.functions import sum as spark_sum, count as spark_count\n",
    "\n",
    "summary = result_df.select(\n",
    "    spark_count(\"Table\").alias(\"total_tables\"),\n",
    "    spark_sum(\"Storage_GB\").alias(\"total_size_gb\"),\n",
    "    spark_sum(\"Row_count\").alias(\"total_rows\"),\n",
    "    spark_sum(\"num_files\").alias(\"total_files\")\n",
    ").first()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Tables:  {summary.total_tables}\")\n",
    "print(f\"Total Size:    {summary.total_size_gb} GB\")\n",
    "print(f\"Total Rows:    {summary.total_rows}\")\n",
    "print(f\"Total Files:   {summary.total_files}\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Get_Table_Sizes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
