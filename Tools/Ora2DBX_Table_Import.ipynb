{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7656a6dd-21ae-4b35-a98b-f814910eaa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Simple Oracle to Databricks Table Replicator\n",
    "# =============================================================================\n",
    "\n",
    "# Oracle connection configuration\n",
    "oracle_config = {\n",
    "    \"jdbc_url\": \"jdbc:oracle:thin:@//pr03db-scan.vs.csin.cz:1521/DWHP\",\n",
    "    \"user\": \"ext98174\",\n",
    "    \"password\": \"Cervenec2025**\",\n",
    "    \"driver\": \"oracle.jdbc.driver.OracleDriver\"\n",
    "}\n",
    "\n",
    "def replicate_oracle_table(ora_schema, ora_table, dbx_schema, include_data=False, overwrite_existing=False):\n",
    "    \"\"\"\n",
    "    Replicate Oracle table structure and optionally data to Databricks\n",
    "    \n",
    "    Args:\n",
    "        ora_schema: Oracle schema name (e.g., 'DWH_OWNER')\n",
    "        ora_table: Oracle table name (e.g., 'USER_CONFIG_PARAMETERS')\n",
    "        dbx_schema: Databricks schema name (e.g., 'dwh_owner')\n",
    "        include_data: Copy data from Oracle (default: False)\n",
    "        overwrite_existing: Drop and recreate if table exists (default: False)\n",
    "    \"\"\"\n",
    "    \n",
    "    target_catalog = \"gap_catalog\"\n",
    "    target_table = f\"{target_catalog}.{dbx_schema}.{ora_table.lower()}\"\n",
    "    \n",
    "    print(f\"=== Replicating {ora_schema}.{ora_table} ‚Üí {target_table} ===\")\n",
    "    \n",
    "    # Create schema if it doesn't exist\n",
    "    try:\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{dbx_schema}\")\n",
    "        print(f\"‚úì Schema {target_catalog}.{dbx_schema} ready\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Schema creation warning: {e}\")\n",
    "    \n",
    "    # Check if table exists\n",
    "    table_exists = False\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE TABLE {target_table}\")\n",
    "        table_exists = True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if table_exists:\n",
    "        if overwrite_existing:\n",
    "            print(f\"üóëÔ∏è  Dropping existing table {target_table}...\")\n",
    "            spark.sql(f\"DROP TABLE {target_table}\")\n",
    "            table_exists = False\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  WARNING: Table {target_table} already exists!\")\n",
    "            if not include_data:\n",
    "                print(\"   Use overwrite_existing=True to recreate, or include_data=True to load data\")\n",
    "                return\n",
    "    \n",
    "    # Create table structure if needed\n",
    "    if not table_exists:\n",
    "        print(f\"üìä Getting structure for {ora_schema}.{ora_table}...\")\n",
    "        \n",
    "        # Get column information from Oracle\n",
    "        column_query = f\"\"\"\n",
    "        (\n",
    "            SELECT \n",
    "                column_name,\n",
    "                data_type,\n",
    "                data_length,\n",
    "                data_precision,\n",
    "                data_scale,\n",
    "                nullable,\n",
    "                column_id\n",
    "            FROM all_tab_columns \n",
    "            WHERE owner = '{ora_schema.upper()}' \n",
    "              AND table_name = '{ora_table.upper()}'\n",
    "            ORDER BY column_id\n",
    "        ) oracle_columns\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            columns_df = spark.read \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", oracle_config[\"jdbc_url\"]) \\\n",
    "                .option(\"dbtable\", column_query) \\\n",
    "                .option(\"user\", oracle_config[\"user\"]) \\\n",
    "                .option(\"password\", oracle_config[\"password\"]) \\\n",
    "                .option(\"driver\", oracle_config[\"driver\"]) \\\n",
    "                .load()\n",
    "            \n",
    "            columns_list = columns_df.collect()\n",
    "            \n",
    "            if not columns_list:\n",
    "                print(f\"‚ùå ERROR: Table {ora_schema}.{ora_table} not found or no access!\")\n",
    "                return\n",
    "            \n",
    "            print(f\"‚úì Found {len(columns_list)} columns\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR getting Oracle table structure: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Convert Oracle data types to Spark data types\n",
    "        def oracle_to_spark_type(ora_type, data_length, data_precision, data_scale):\n",
    "            ora_type = ora_type.upper()\n",
    "            \n",
    "            if ora_type == 'NUMBER':\n",
    "                if data_precision and data_scale is not None:\n",
    "                    if data_scale == 0:\n",
    "                        # Integer types\n",
    "                        if data_precision <= 9:\n",
    "                            return 'INT'\n",
    "                        elif data_precision <= 18:\n",
    "                            return 'BIGINT'\n",
    "                        else:\n",
    "                            return f'DECIMAL({data_precision},0)'\n",
    "                    else:\n",
    "                        # Decimal types\n",
    "                        return f'DECIMAL({data_precision},{data_scale})'\n",
    "                else:\n",
    "                    # NUMBER without precision - common for IDs/keys, use BIGINT\n",
    "                    return 'BIGINT'\n",
    "            \n",
    "            type_mapping = {\n",
    "                'VARCHAR2': 'STRING',\n",
    "                'CHAR': 'STRING',\n",
    "                'NCHAR': 'STRING', \n",
    "                'NVARCHAR2': 'STRING',\n",
    "                'CLOB': 'STRING',\n",
    "                'DATE': 'TIMESTAMP',\n",
    "                'TIMESTAMP': 'TIMESTAMP',\n",
    "                'BLOB': 'BINARY',\n",
    "                'RAW': 'BINARY'\n",
    "            }\n",
    "            \n",
    "            return type_mapping.get(ora_type, 'STRING')\n",
    "        \n",
    "        # Build CREATE TABLE statement\n",
    "        print(\"üîß Building Databricks table structure...\")\n",
    "        column_definitions = []\n",
    "        \n",
    "        for col in columns_list:\n",
    "            col_name = col['COLUMN_NAME'].lower()\n",
    "            ora_type = col['DATA_TYPE']\n",
    "            data_length = col['DATA_LENGTH']\n",
    "            data_precision = col['DATA_PRECISION']\n",
    "            data_scale = col['DATA_SCALE']\n",
    "            nullable = col['NULLABLE'] == 'Y'\n",
    "            \n",
    "            spark_type = oracle_to_spark_type(ora_type, data_length, data_precision, data_scale)\n",
    "            null_constraint = \"\" if nullable else \" NOT NULL\"\n",
    "            col_def = f\"    {col_name} {spark_type}{null_constraint}\"\n",
    "            column_definitions.append(col_def)\n",
    "            \n",
    "            print(f\"  {col_name}: {ora_type} ‚Üí {spark_type}{null_constraint}\")\n",
    "        \n",
    "        # Create the table\n",
    "        columns_text = ',\\n'.join(column_definitions)\n",
    "        create_table_sql = f\"\"\"\n",
    "        CREATE TABLE {target_table} (\n",
    "{columns_text}\n",
    "        )\n",
    "        USING DELTA\n",
    "        COMMENT 'Replicated from Oracle {ora_schema}.{ora_table}'\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            spark.sql(create_table_sql)\n",
    "            print(f\"‚úÖ SUCCESS: Table {target_table} created!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR creating table: {e}\")\n",
    "            return\n",
    "    \n",
    "    # Copy data if requested\n",
    "    if include_data:\n",
    "        print(f\"üìä Copying data from Oracle...\")\n",
    "        \n",
    "        try:\n",
    "            # Read data from Oracle\n",
    "            data_df = spark.read \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", oracle_config[\"jdbc_url\"]) \\\n",
    "                .option(\"dbtable\", f\"{ora_schema}.{ora_table}\") \\\n",
    "                .option(\"user\", oracle_config[\"user\"]) \\\n",
    "                .option(\"password\", oracle_config[\"password\"]) \\\n",
    "                .option(\"driver\", oracle_config[\"driver\"]) \\\n",
    "                .load()\n",
    "            \n",
    "            row_count = data_df.count()\n",
    "            print(f\"‚úì Found {row_count} rows to copy\")\n",
    "            \n",
    "            if row_count > 0:\n",
    "                # Fix case sensitivity - convert column names to lowercase\n",
    "                print(\"üîß Converting column names to lowercase...\")\n",
    "                for col_name in data_df.columns:\n",
    "                    data_df = data_df.withColumnRenamed(col_name, col_name.lower())\n",
    "                \n",
    "                # For existing tables, clear data first to avoid schema conflicts\n",
    "                if table_exists and not overwrite_existing:\n",
    "                    print(\"üßπ Clearing existing table data...\")\n",
    "                    spark.sql(f\"DELETE FROM {target_table}\")\n",
    "                \n",
    "                # Write data to Databricks table  \n",
    "                if table_exists and not overwrite_existing:\n",
    "                    # Use insertInto for existing tables\n",
    "                    data_df.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"append\") \\\n",
    "                        .insertInto(target_table)\n",
    "                else:\n",
    "                    # Use saveAsTable with explicit overwrite for new/overwritten tables\n",
    "                    data_df.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"overwriteSchema\", \"true\") \\\n",
    "                        .saveAsTable(target_table)\n",
    "                \n",
    "                print(f\"‚úÖ SUCCESS: Loaded {row_count} rows into {target_table}\")\n",
    "                \n",
    "                # Show sample data\n",
    "                print(\"üìã Sample data:\")\n",
    "                spark.sql(f\"SELECT * FROM {target_table} LIMIT 3\").show()\n",
    "                \n",
    "                # Show final row count\n",
    "                final_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {target_table}\").collect()[0][0]\n",
    "                print(f\"üìä Final table contains {final_count} rows\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ÑπÔ∏è  No data to copy (empty table)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR copying data: {e}\")\n",
    "            print(\"üí° Try using overwrite_existing=True to recreate the table\")\n",
    "\n",
    "# 1. Create table structure only:'\n",
    "# replicate_oracle_table(\"DWH_OWNER\", \"USER_CONFIG_PARAMETERS\", \"dwh_owner\")')\n",
    "\n",
    "# 2. Create table with data:'\n",
    "replicate_oracle_table(\"ADS_OWNER\", \"EVENT_STATUS\", \"ads_owner\", include_data=True, overwrite_existing=True)\n",
    "\n",
    "#replicate_oracle_table(\"ADS_ETL_OWNER\", \"DLK_ADS_LOV_RDS_ANALYTICALEVENTSTATUS\", \"ads_etl_owner\", include_data=True, overwrite_existing=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ora2DBX_Table_Import",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
