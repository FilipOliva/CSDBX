{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0298bc4-305a-40f9-94f9-6745e1cd6123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"hours_back\", \"24\", \"Hours Back\")\n",
    "dbutils.widgets.text(\"TARGET_TAG_KEY\", \"etl\", \"Target Tag Key\")\n",
    "dbutils.widgets.text(\"TARGET_TAG_VALUES\", \"ADS_RDS\", \"Target Tag Values (comma-separated)\")\n",
    "dbutils.widgets.text(\"CATALOG\", \"gap_catalog\", \"Target Catalog\")\n",
    "dbutils.widgets.text(\"SCHEMA\", \"ads_owner\", \"Target Schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3497a72e-931c-46dc-b183-cdad9586eea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Read widget values\n",
    "hours_back        = int(dbutils.widgets.get(\"hours_back\"))\n",
    "TARGET_TAG_KEY    = dbutils.widgets.get(\"TARGET_TAG_KEY\")\n",
    "TARGET_TAG_VALUES = [v.strip() for v in dbutils.widgets.get(\"TARGET_TAG_VALUES\").split(\",\") if v.strip()]\n",
    "CATALOG           = dbutils.widgets.get(\"CATALOG\")\n",
    "SCHEMA            = dbutils.widgets.get(\"SCHEMA\")\n",
    "\n",
    "RUN_TABLE  = f\"{CATALOG}.{SCHEMA}.job_run_costs\"\n",
    "TASK_TABLE = f\"{CATALOG}.{SCHEMA}.job_run_task_costs\"\n",
    "TARGET_CURRENCY  = \"USD\"\n",
    "\n",
    "# =========================================\n",
    "# Helpers\n",
    "# =========================================\n",
    "def table_exists(catalog: str, schema: str, name: str) -> bool:\n",
    "    return (\n",
    "        spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\")\n",
    "             .filter(F.col(\"tableName\") == name)\n",
    "             .limit(1).count() > 0\n",
    "    )\n",
    "\n",
    "def current_prices(currency: str):\n",
    "    # Unit price = COALESCE(promotional.default, effective_list.default, default)\n",
    "    raw = (\n",
    "        spark.read.table(\"system.billing.list_prices\")\n",
    "        .filter(F.col(\"currency_code\") == currency)\n",
    "        .filter(\n",
    "            (F.col(\"price_start_time\") <= F.current_timestamp()) &\n",
    "            (F.col(\"price_end_time\").isNull() | (F.col(\"price_end_time\") >= F.current_timestamp()))\n",
    "        )\n",
    "        .select(\n",
    "            \"sku_name\", \"price_start_time\",\n",
    "            F.coalesce(\n",
    "                F.col(\"pricing.promotional.default\"),\n",
    "                F.col(\"pricing.effective_list.default\"),\n",
    "                F.col(\"pricing.default\")\n",
    "            ).cast(\"double\").alias(\"unit_price\")\n",
    "        )\n",
    "    )\n",
    "    w = Window.partitionBy(\"sku_name\").orderBy(F.col(\"price_start_time\").desc_nulls_last())\n",
    "    return (\n",
    "        raw.withColumn(\"rn\", F.row_number().over(w))\n",
    "           .filter(\"rn = 1\")\n",
    "           .select(F.col(\"sku_name\").alias(\"price_sku_name\"),\n",
    "                   F.col(\"unit_price\").alias(\"dbu_rate\"))\n",
    "    )\n",
    "\n",
    "# =========================================\n",
    "# Cutoff & tag prep\n",
    "# =========================================\n",
    "cutoff_ts = F.expr(f\"current_timestamp() - INTERVAL {hours_back} HOURS\")\n",
    "now_ts    = F.current_timestamp()\n",
    "vals_lc   = [v.lower() for v in TARGET_TAG_VALUES]\n",
    "\n",
    "# =========================================\n",
    "# Tagged jobs\n",
    "# =========================================\n",
    "df_jobs_tagged = (\n",
    "    spark.read.table(\"system.lakeflow.jobs\")\n",
    "    .select(\"workspace_id\", \"job_id\", \"name\", \"tags\")\n",
    "    .withColumn(\"tag_val_lc\", F.lower(F.element_at(\"tags\", F.lit(TARGET_TAG_KEY))))\n",
    "    .filter(F.col(\"tag_val_lc\").isin(vals_lc))\n",
    "    .select(\"workspace_id\", \"job_id\", \"name\")\n",
    "    .dropDuplicates([\"workspace_id\", \"job_id\"])\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# RUNS (workflow-level) — aggregate slices -> 1 row/run\n",
    "# Use overlap filter so we don't miss runs that started earlier\n",
    "# =========================================\n",
    "df_run_slices = (\n",
    "    spark.read.table(\"system.lakeflow.job_run_timeline\")\n",
    "    .filter(\n",
    "        (F.col(\"period_start_time\") <= now_ts) &\n",
    "        (F.col(\"period_end_time\").isNull() | (F.col(\"period_end_time\") >= cutoff_ts))\n",
    "    )\n",
    "    .select(\"workspace_id\", \"job_id\", \"run_id\",\n",
    "            \"period_start_time\", \"period_end_time\", \"result_state\")\n",
    "    .withColumnRenamed(\"result_state\", \"status\")\n",
    "    .join(df_jobs_tagged.select(\"workspace_id\",\"job_id\",\"name\"), [\"workspace_id\",\"job_id\"], \"inner\")\n",
    ")\n",
    "\n",
    "df_runs = (\n",
    "    df_run_slices\n",
    "    .groupBy(\"workspace_id\",\"job_id\",\"run_id\",\"name\")\n",
    "    .agg(\n",
    "        F.min(\"period_start_time\").alias(\"period_start_time\"),\n",
    "        F.max(\"period_end_time\").alias(\"period_end_time\"),\n",
    "        F.max(\"status\").alias(\"status\")\n",
    "    )\n",
    "    .withColumn(\"period_end_time\", F.coalesce(\"period_end_time\", now_ts))\n",
    "    .withColumn(\n",
    "        \"duration_minutes\",\n",
    "        (F.col(\"period_end_time\").cast(\"double\") - F.col(\"period_start_time\").cast(\"double\")) / 60.0\n",
    "    )\n",
    "    .withColumn(\"job_name\", F.coalesce(F.col(\"name\"), F.concat(F.lit(\"job_id=\"), F.col(\"job_id\"))))\n",
    "    .select(\"workspace_id\",\"job_id\",\"run_id\",\"period_start_time\",\"period_end_time\",\"status\",\"duration_minutes\",\"job_name\")\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# Usage & costs (run-level)\n",
    "# =========================================\n",
    "df_prices = current_prices(TARGET_CURRENCY)\n",
    "\n",
    "df_usage = (\n",
    "    spark.read.table(\"system.billing.usage\")\n",
    "    .select(\n",
    "        F.col(\"workspace_id\"),\n",
    "        F.col(\"usage_metadata.job_run_id\").cast(\"string\").alias(\"run_id\"),\n",
    "        F.col(\"sku_name\"),\n",
    "        F.col(\"usage_quantity\").cast(\"double\").alias(\"usage_quantity\")\n",
    "    )\n",
    "    .filter(\"run_id IS NOT NULL\")\n",
    ")\n",
    "\n",
    "df_billing = (\n",
    "    df_usage\n",
    "    .join(df_prices, df_usage.sku_name == df_prices.price_sku_name, \"left\")\n",
    "    .groupBy(\"run_id\")\n",
    "    .agg(\n",
    "        F.round(F.sum(F.col(\"usage_quantity\") * F.coalesce(F.col(\"dbu_rate\"), F.lit(0.0))), 6)\n",
    "            .alias(\"total_cost_usd\"),\n",
    "        F.round(F.sum(\"usage_quantity\"), 6).alias(\"total_dbus_used\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# TASK RUNS (subtasks) — aggregate slices -> 1 row per (run_id, task_key)\n",
    "# Use overlap filter like runs\n",
    "# =========================================\n",
    "df_task_slices = (\n",
    "    spark.read.table(\"system.lakeflow.job_task_run_timeline\")\n",
    "    .filter(\n",
    "        (F.col(\"period_start_time\") <= now_ts) &\n",
    "        (F.col(\"period_end_time\").isNull() | (F.col(\"period_end_time\") >= cutoff_ts))\n",
    "    )\n",
    "    .select(\"workspace_id\", \"job_id\", \"run_id\", \"job_run_id\", \"task_key\",\n",
    "            \"period_start_time\", \"period_end_time\", \"result_state\")\n",
    "    .withColumnRenamed(\"result_state\", \"status\")\n",
    "    .withColumn(\"period_end_time\", F.coalesce(\"period_end_time\", now_ts))\n",
    "    .withColumn(\n",
    "        \"slice_minutes\",\n",
    "        (F.col(\"period_end_time\").cast(\"double\") - F.col(\"period_start_time\").cast(\"double\")) / 60.0\n",
    "    )\n",
    ")\n",
    "\n",
    "df_task_durations = (\n",
    "    df_task_slices\n",
    "    .groupBy(\"workspace_id\",\"job_id\",\"job_run_id\",\"task_key\")\n",
    "    .agg(\n",
    "        F.sum(\"slice_minutes\").alias(\"task_duration_minutes\"),\n",
    "        F.max(\"status\").alias(\"status\")\n",
    "    )\n",
    "    .withColumnRenamed(\"job_run_id\", \"run_id\")\n",
    ")\n",
    "\n",
    "# Keep only tasks for surfaced runs\n",
    "df_task_durations = df_task_durations.join(\n",
    "    df_runs.select(\"workspace_id\",\"job_id\",\"run_id\").distinct(),\n",
    "    [\"workspace_id\",\"job_id\",\"run_id\"], \"inner\"\n",
    ").dropDuplicates([\"run_id\",\"task_key\"])\n",
    "\n",
    "# =========================================\n",
    "# Allocate run costs to tasks by duration share\n",
    "# =========================================\n",
    "task_minutes_per_run = (\n",
    "    df_task_durations\n",
    "    .groupBy(\"run_id\")\n",
    "    .agg(F.sum(F.col(\"task_duration_minutes\")).alias(\"run_task_minutes\"))\n",
    ")\n",
    "\n",
    "alloc_with_totals = (\n",
    "    df_task_durations\n",
    "    .join(task_minutes_per_run, \"run_id\", \"left\")\n",
    "    .withColumn(\n",
    "        \"duration_share\",\n",
    "        F.when(F.col(\"run_task_minutes\") > 0,\n",
    "               F.col(\"task_duration_minutes\") / F.col(\"run_task_minutes\")).otherwise(F.lit(0.0))\n",
    "    )\n",
    "    .join(df_billing, \"run_id\", \"left\")\n",
    "    .withColumn(\"total_dbus_used\", F.coalesce(F.col(\"total_dbus_used\"), F.lit(0.0)))\n",
    "    .withColumn(\"total_cost_usd\", F.coalesce(F.col(\"total_cost_usd\"), F.lit(0.0)))\n",
    "    .withColumn(\"task_dbus_used\", F.round(F.col(\"duration_share\") * F.col(\"total_dbus_used\"), 6))\n",
    "    .withColumn(\"task_cost_usd\", F.round(F.col(\"duration_share\") * F.col(\"total_cost_usd\"), 6))\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# Final results (RUN / TASK)\n",
    "# =========================================\n",
    "df_run_result = (\n",
    "    df_runs\n",
    "    .join(df_billing, \"run_id\", \"left\")\n",
    "    .withColumn(\"total_dbus_used\", F.coalesce(F.col(\"total_dbus_used\"), F.lit(0.0)))\n",
    "    .withColumn(\"total_cost_usd\", F.coalesce(F.col(\"total_cost_usd\"), F.lit(0.0)))\n",
    "    .select(\n",
    "        \"period_start_time\",\n",
    "        \"workspace_id\",\n",
    "        \"job_id\",\n",
    "        \"run_id\",\n",
    "        \"job_name\",\n",
    "        \"status\",\n",
    "        F.round(\"duration_minutes\", 2).alias(\"duration_minutes\"),\n",
    "        \"total_dbus_used\",\n",
    "        \"total_cost_usd\",\n",
    "    )\n",
    ")\n",
    "\n",
    "df_task_result = (\n",
    "    alloc_with_totals\n",
    "    .select(\n",
    "        \"workspace_id\",\n",
    "        \"job_id\",\n",
    "        \"run_id\",\n",
    "        \"task_key\",\n",
    "        \"status\",\n",
    "        F.round(\"task_duration_minutes\", 2).alias(\"task_duration_minutes\"),\n",
    "        F.round(\"duration_share\", 6).alias(\"duration_share\"),\n",
    "        F.round(\"task_dbus_used\", 6).alias(\"task_dbus_used\"),\n",
    "        F.round(\"task_cost_usd\", 6).alias(\"task_cost_usd\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# =========================================\n",
    "# Persist to Delta (serverless-safe) with MERGE\n",
    "#   - De-dup sources BEFORE MERGE to avoid conflicts\n",
    "# =========================================\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# Dedup runs: 1 row/run_id\n",
    "w_run = Window.partitionBy(\"run_id\").orderBy(F.col(\"period_start_time\").desc())\n",
    "df_run_result_dedup = (\n",
    "    df_run_result\n",
    "    .withColumn(\"rn\", F.row_number().over(w_run))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    "    .withColumn(\"load_ts\", F.current_timestamp())\n",
    "    .withColumn(\"load_date\", F.to_date(\"period_start_time\"))\n",
    "    .withColumn(\"window_hours\", F.lit(hours_back))\n",
    ")\n",
    "\n",
    "# Dedup tasks: 1 row/(run_id,task_key)\n",
    "w_task = Window.partitionBy(\"run_id\",\"task_key\").orderBy(F.col(\"task_duration_minutes\").desc())\n",
    "df_task_result_dedup = (\n",
    "    df_task_result\n",
    "    .withColumn(\"rn\", F.row_number().over(w_task))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# Attach run start date for task partitioning\n",
    "df_task_result_with_date = (\n",
    "    df_task_result_dedup.join(\n",
    "        df_run_result_dedup.select(\"run_id\", \"period_start_time\"),\n",
    "        \"run_id\", \"left\"\n",
    "    )\n",
    "    .withColumn(\"load_ts\", F.current_timestamp())\n",
    "    .withColumn(\"load_date\", F.to_date(\"period_start_time\"))\n",
    "    .withColumn(\"window_hours\", F.lit(hours_back))\n",
    "    .drop(\"period_start_time\")\n",
    ")\n",
    "\n",
    "# Save runs\n",
    "if not table_exists(CATALOG, SCHEMA, \"job_run_costs\"):\n",
    "    (df_run_result_dedup\n",
    "        .write.format(\"delta\").mode(\"overwrite\").partitionBy(\"load_date\")\n",
    "        .saveAsTable(RUN_TABLE))\n",
    "else:\n",
    "    tgt = DeltaTable.forName(spark, RUN_TABLE)\n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(df_run_result_dedup.alias(\"s\"), \"t.run_id = s.run_id\")\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute())\n",
    "\n",
    "# Save tasks (key = run_id + task_key)\n",
    "if not table_exists(CATALOG, SCHEMA, \"job_run_task_costs\"):\n",
    "    (df_task_result_with_date\n",
    "        .write.format(\"delta\").mode(\"overwrite\").partitionBy(\"load_date\")\n",
    "        .saveAsTable(TASK_TABLE))\n",
    "else:\n",
    "    tgt = DeltaTable.forName(spark, TASK_TABLE)\n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(df_task_result_with_date.alias(\"s\"),\n",
    "               \"t.run_id = s.run_id AND t.task_key = s.task_key\")\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute())\n",
    "\n",
    "print(\"Saved results.\")\n",
    "# Optional quick peek:\n",
    "display(spark.table(RUN_TABLE).orderBy(F.col(\"period_start_time\").desc()).limit(20))\n",
    "display(spark.table(TASK_TABLE).orderBy(F.col(\"run_id\").desc(), F.col(\"task_key\")).limit(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4632f34-d818-4105-9043-a975d267e3f1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755686879456}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---------- Parameters ----------\n",
    "dbutils.widgets.text(\"CATALOG\", \"\")\n",
    "dbutils.widgets.text(\"SCHEMA\", \"\")\n",
    "dbutils.widgets.text(\"START_DATE\", \"\")\n",
    "dbutils.widgets.text(\"END_DATE\", \"\")\n",
    "dbutils.widgets.dropdown(\"WRITE_STATS\", \"false\", [\"false\",\"true\"])\n",
    "\n",
    "CATALOG     = dbutils.widgets.get(\"CATALOG\").strip()\n",
    "SCHEMA      = dbutils.widgets.get(\"SCHEMA\").strip()\n",
    "START_DATE  = dbutils.widgets.get(\"START_DATE\").strip()\n",
    "END_DATE    = dbutils.widgets.get(\"END_DATE\").strip()\n",
    "WRITE_STATS = dbutils.widgets.get(\"WRITE_STATS\") == \"true\"\n",
    "\n",
    "RUN_TABLE  = f\"{CATALOG}.{SCHEMA}.job_run_costs\"\n",
    "TASK_TABLE = f\"{CATALOG}.{SCHEMA}.job_run_task_costs\"\n",
    "\n",
    "OUT_TASK   = f\"{CATALOG}.{SCHEMA}.task_cost_stats\"\n",
    "OUT_RUN    = f\"{CATALOG}.{SCHEMA}.job_run_stats\"\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def nz_div(num_col, den_col):\n",
    "    return F.when(den_col.isNull() | (den_col == 0), F.lit(0.0)).otherwise(num_col / den_col)\n",
    "\n",
    "def q(col, probs=(0.5, 0.9), accuracy=10000):\n",
    "    return F.expr(f\"percentile_approx({col}, array({','.join(map(str, probs))}), {accuracy})\")\n",
    "\n",
    "# ---------- Load ----------\n",
    "df_tasks = spark.read.table(TASK_TABLE)\n",
    "df_runs  = spark.read.table(RUN_TABLE)\n",
    "\n",
    "# Optional date filters (by load_date)\n",
    "if START_DATE:\n",
    "    df_tasks = df_tasks.filter(F.col(\"load_date\") >= F.to_date(F.lit(START_DATE)))\n",
    "    df_runs  = df_runs.filter(F.col(\"load_date\")  >= F.to_date(F.lit(START_DATE)))\n",
    "if END_DATE:\n",
    "    df_tasks = df_tasks.filter(F.col(\"load_date\") <= F.to_date(F.lit(END_DATE)))\n",
    "    df_runs  = df_runs.filter(F.col(\"load_date\")  <= F.to_date(F.lit(END_DATE)))\n",
    "\n",
    "# ---------- task_key rollup ----------\n",
    "task_stats = (\n",
    "    df_tasks.groupBy(\"task_key\")\n",
    "    .agg(\n",
    "        F.count(F.lit(1)).alias(\"n_runs\"),\n",
    "        F.sum(F.when(F.col(\"status\")==\"SUCCEEDED\",1).otherwise(0)).alias(\"n_success\"),\n",
    "        F.sum(\"task_duration_minutes\").alias(\"total_minutes\"),\n",
    "        F.avg(\"task_duration_minutes\").alias(\"avg_minutes\"),\n",
    "        F.min(\"task_duration_minutes\").alias(\"min_minutes\"),\n",
    "        F.max(\"task_duration_minutes\").alias(\"max_minutes\"),\n",
    "        F.stddev_pop(\"task_duration_minutes\").alias(\"std_minutes\"),\n",
    "        q(\"task_duration_minutes\").alias(\"q_minutes\"),\n",
    "        F.sum(\"task_cost_usd\").alias(\"total_cost_usd\"),\n",
    "        F.avg(\"task_cost_usd\").alias(\"avg_cost_usd\"),\n",
    "        F.min(\"task_cost_usd\").alias(\"min_cost_usd\"),\n",
    "        F.max(\"task_cost_usd\").alias(\"max_cost_usd\"),\n",
    "        F.stddev_pop(\"task_cost_usd\").alias(\"std_cost_usd\"),\n",
    "        q(\"task_cost_usd\").alias(\"q_cost_usd\"),\n",
    "        F.sum(\"task_dbus_used\").alias(\"total_dbus\"),\n",
    "        F.avg(\"task_dbus_used\").alias(\"avg_dbus\"),\n",
    "        F.stddev_pop(\"task_dbus_used\").alias(\"std_dbus\"),\n",
    "        q(\"task_dbus_used\").alias(\"q_dbus\"),\n",
    "        F.avg(\"duration_share\").alias(\"avg_duration_share\"),\n",
    "        F.max(\"load_ts\").alias(\"last_load_ts\")\n",
    "    )\n",
    "    .withColumn(\"success_rate\", nz_div(F.col(\"n_success\").cast(\"double\"), F.col(\"n_runs\").cast(\"double\")))\n",
    "    .withColumn(\"median_minutes\", F.col(\"q_minutes\")[0]).withColumn(\"p90_minutes\", F.col(\"q_minutes\")[1])\n",
    "    .withColumn(\"median_cost_usd\", F.col(\"q_cost_usd\")[0]).withColumn(\"p90_cost_usd\", F.col(\"q_cost_usd\")[1])\n",
    "    .withColumn(\"median_dbus\", F.col(\"q_dbus\")[0]).withColumn(\"p90_dbus\", F.col(\"q_dbus\")[1])\n",
    "    .drop(\"q_minutes\",\"q_cost_usd\",\"q_dbus\")\n",
    "    .withColumn(\"avg_cost_per_min\", nz_div(F.col(\"total_cost_usd\"), F.col(\"total_minutes\")))\n",
    "    .withColumn(\"avg_dbus_per_min\", nz_div(F.col(\"total_dbus\"), F.col(\"total_minutes\")))\n",
    "    .withColumn(\"cv_minutes\", nz_div(F.col(\"std_minutes\"), F.col(\"avg_minutes\")))\n",
    "    .withColumn(\"cv_cost_usd\", nz_div(F.col(\"std_cost_usd\"), F.col(\"avg_cost_usd\")))\n",
    ")\n",
    "\n",
    "# Optional rounding for readability (keep raw if you prefer)\n",
    "task_stats = (\n",
    "    task_stats\n",
    "    .select(\n",
    "        \"task_key\",\"n_runs\",\"n_success\",\n",
    "        F.round(\"total_minutes\",6).alias(\"total_minutes\"),\n",
    "        F.round(\"avg_minutes\",6).alias(\"avg_minutes\"),\n",
    "        F.round(\"min_minutes\",6).alias(\"min_minutes\"),\n",
    "        F.round(\"max_minutes\",6).alias(\"max_minutes\"),\n",
    "        F.round(\"std_minutes\",6).alias(\"std_minutes\"),\n",
    "        F.round(\"median_minutes\",6).alias(\"median_minutes\"),\n",
    "        F.round(\"p90_minutes\",6).alias(\"p90_minutes\"),\n",
    "        F.round(\"total_cost_usd\",6).alias(\"total_cost_usd\"),\n",
    "        F.round(\"avg_cost_usd\",6).alias(\"avg_cost_usd\"),\n",
    "        F.round(\"min_cost_usd\",6).alias(\"min_cost_usd\"),\n",
    "        F.round(\"max_cost_usd\",6).alias(\"max_cost_usd\"),\n",
    "        F.round(\"std_cost_usd\",6).alias(\"std_cost_usd\"),\n",
    "        F.round(\"median_cost_usd\",6).alias(\"median_cost_usd\"),\n",
    "        F.round(\"p90_cost_usd\",6).alias(\"p90_cost_usd\"),\n",
    "        F.round(\"total_dbus\",6).alias(\"total_dbus\"),\n",
    "        F.round(\"avg_dbus\",6).alias(\"avg_dbus\"),\n",
    "        F.round(\"std_dbus\",6).alias(\"std_dbus\"),\n",
    "        F.round(\"median_dbus\",6).alias(\"median_dbus\"),\n",
    "        F.round(\"p90_dbus\",6).alias(\"p90_dbus\"),\n",
    "        F.round(\"avg_duration_share\",6).alias(\"avg_duration_share\"),\n",
    "        \"last_load_ts\",\n",
    "        F.round(\"success_rate\",6).alias(\"success_rate\"),\n",
    "        F.round(\"avg_cost_per_min\",12).alias(\"avg_cost_per_min\"),\n",
    "        F.round(\"avg_dbus_per_min\",12).alias(\"avg_dbus_per_min\"),\n",
    "        F.round(\"cv_minutes\",12).alias(\"cv_minutes\"),\n",
    "        F.round(\"cv_cost_usd\",12).alias(\"cv_cost_usd\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------- run-level job stats ----------\n",
    "run_aggs = (\n",
    "    df_runs.groupBy(\"workspace_id\",\"job_id\",\"job_name\")\n",
    "    .agg(\n",
    "        F.count(F.lit(1)).alias(\"n_runs\"),\n",
    "        F.sum(F.when(F.col(\"status\")==\"SUCCEEDED\",1).otherwise(0)).alias(\"n_success\"),\n",
    "        F.sum(\"duration_minutes\").alias(\"total_run_minutes\"),\n",
    "        F.avg(\"duration_minutes\").alias(\"avg_run_minutes\"),\n",
    "        F.expr(\"percentile_approx(duration_minutes, array(0.5, 0.9), 10000)\").alias(\"q_run_minutes\"),\n",
    "        F.sum(\"total_cost_usd\").alias(\"total_run_cost_usd\"),\n",
    "        F.avg(\"total_cost_usd\").alias(\"avg_run_cost_usd\"),\n",
    "        F.expr(\"percentile_approx(total_cost_usd, array(0.5, 0.9), 10000)\").alias(\"q_run_cost_usd\"),\n",
    "        F.sum(\"total_dbus_used\").alias(\"total_run_dbus\"),\n",
    "        F.avg(\"total_dbus_used\").alias(\"avg_run_dbus\"),\n",
    "        F.max(\"load_ts\").alias(\"last_load_ts\")\n",
    "    )\n",
    "    .withColumn(\"success_rate\", nz_div(F.col(\"n_success\").cast(\"double\"), F.col(\"n_runs\").cast(\"double\")))\n",
    "    .withColumn(\"median_run_minutes\", F.col(\"q_run_minutes\")[0]).withColumn(\"p90_run_minutes\", F.col(\"q_run_minutes\")[1])\n",
    "    .withColumn(\"median_run_cost_usd\", F.col(\"q_run_cost_usd\")[0]).withColumn(\"p90_run_cost_usd\", F.col(\"q_run_cost_usd\")[1])\n",
    "    .drop(\"q_run_minutes\",\"q_run_cost_usd\")\n",
    "    .withColumn(\"avg_run_cost_per_min\", nz_div(F.col(\"total_run_cost_usd\"), F.col(\"total_run_minutes\")))\n",
    "    .withColumn(\"avg_run_dbus_per_min\", nz_div(F.col(\"total_run_dbus\"), F.col(\"total_run_minutes\")))\n",
    ")\n",
    "\n",
    "# Optional rounding\n",
    "run_aggs = (\n",
    "    run_aggs\n",
    "    .select(\n",
    "        \"workspace_id\",\"job_id\",\"job_name\",\"n_runs\",\"n_success\",\n",
    "        F.round(\"total_run_minutes\",6).alias(\"total_run_minutes\"),\n",
    "        F.round(\"avg_run_minutes\",6).alias(\"avg_run_minutes\"),\n",
    "        F.round(\"median_run_minutes\",6).alias(\"median_run_minutes\"),\n",
    "        F.round(\"p90_run_minutes\",6).alias(\"p90_run_minutes\"),\n",
    "        F.round(\"total_run_cost_usd\",6).alias(\"total_run_cost_usd\"),\n",
    "        F.round(\"avg_run_cost_usd\",6).alias(\"avg_run_cost_usd\"),\n",
    "        F.round(\"median_run_cost_usd\",6).alias(\"median_run_cost_usd\"),\n",
    "        F.round(\"p90_run_cost_usd\",6).alias(\"p90_run_cost_usd\"),\n",
    "        F.round(\"total_run_dbus\",6).alias(\"total_run_dbus\"),\n",
    "        F.round(\"avg_run_dbus\",6).alias(\"avg_run_dbus\"),\n",
    "        \"last_load_ts\",\n",
    "        F.round(\"success_rate\",6).alias(\"success_rate\"),\n",
    "        F.round(\"avg_run_cost_per_min\",12).alias(\"avg_run_cost_per_min\"),\n",
    "        F.round(\"avg_run_dbus_per_min\",12).alias(\"avg_run_dbus_per_min\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------- (optional) Persist ----------\n",
    "# if WRITE_STATS:\n",
    "#     spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "#     task_stats.write.mode(\"overwrite\").format(\"delta\").saveAsTable(OUT_TASK)\n",
    "#     run_aggs.write.mode(\"overwrite\").format(\"delta\").saveAsTable(OUT_RUN)\n",
    "\n",
    "# ---------- Displays ----------\n",
    "display(task_stats.orderBy(F.col(\"total_cost_usd\").desc()).limit(50))\n",
    "display(run_aggs.orderBy(F.col(\"total_run_cost_usd\").desc()).limit(50))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DBX_Measuring_TC",
   "widgets": {
    "CATALOG": {
     "currentValue": "gap_catalog",
     "nuid": "af58f350-77c6-4d93-84e9-6f3b3e288e72",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "CATALOG",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "CATALOG",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "END_DATE": {
     "currentValue": "",
     "nuid": "09e324f4-a82f-40d1-a3f9-4b14ecba7f6f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "END_DATE",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "END_DATE",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "SCHEMA": {
     "currentValue": "default",
     "nuid": "5ddb8f08-4fab-4b4b-9f74-b0c6223c341b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "SCHEMA",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "SCHEMA",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "START_DATE": {
     "currentValue": "",
     "nuid": "840af3cf-1bc0-494b-a5c3-0bb3193e30c7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "START_DATE",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "START_DATE",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "TARGET_TAG_KEY": {
     "currentValue": "etl",
     "nuid": "8ef1be78-b7c7-429c-a907-a5cecde7dd2c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "etl",
      "label": "Target Tag Key",
      "name": "TARGET_TAG_KEY",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "etl",
      "label": "Target Tag Key",
      "name": "TARGET_TAG_KEY",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "TARGET_TAG_VALUES": {
     "currentValue": "ads_rds",
     "nuid": "c9cb0ec8-3e5f-433a-89e7-128c74376d3d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ADS_RDS",
      "label": "Target Tag Values (comma-separated)",
      "name": "TARGET_TAG_VALUES",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ADS_RDS",
      "label": "Target Tag Values (comma-separated)",
      "name": "TARGET_TAG_VALUES",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "WRITE_STATS": {
     "currentValue": "false",
     "nuid": "ba8e94d5-8767-4877-a8b8-df93362e7336",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": null,
      "name": "WRITE_STATS",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "false",
        "true"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": null,
      "name": "WRITE_STATS",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "false",
        "true"
       ]
      }
     }
    },
    "hours_back": {
     "currentValue": "8",
     "nuid": "66117ba7-9697-4e7b-9ead-79c775a74dd8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "24",
      "label": "Hours Back",
      "name": "hours_back",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "24",
      "label": "Hours Back",
      "name": "hours_back",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
