{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 10485760,
      "rowLimit": 1000
     },
     "inputWidgets": {},
     "nuid": "f1e665dc-1041-44e0-93ae-dde65f0becc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "select * from gap_catalog.ads_owner.EVENT_STATUS\n",
    "where est_key||'.'||est_valid_from = '1.2025-08-14 00:00:00'\n",
    "order by est_key;\n",
    "\n",
    "select * from\n",
    "gap_catalog.ads_etl_owner.XC_RDS_EVENT_STATUS_DISTRAINT_ACT_STATE;\n",
    "\n",
    "select * from\n",
    "gap_catalog.ads_etl_owner.DIFF_ADS_RDS_EVENT_STATUS_DISTRAINT_ACT_STATE_ADS_MAP_SCD_DIFF\n",
    "where tech_rid is not null;\n",
    "\n",
    "---------------------------------------------------------------------------------------------\n",
    "select xxhash64(uuid() ) \n",
    "\n",
    "SELECT \n",
    "*\n",
    "FROM system.compute.warehouses \n",
    "WHERE warehouse_id = current_warehouse();\n",
    "\n",
    "select new_customer_guid;\n",
    "select current_warehouse();\n",
    "\n",
    "SELECT as current_user;\n",
    "---------------------------------------------------------------------------------------------\n",
    "-- Disable Adaptive Query Execution (AQE) for small data\n",
    "SET spark.sql.adaptive.enabled = false;\n",
    "SET spark.sql.adaptive.coalescePartitions.enabled = false;\n",
    "SET spark.sql.adaptive.localShuffleReader.enabled = false;\n",
    "SET spark.sql.adaptive.skewJoin.enabled = false;\n",
    "\n",
    "-- Reduce shuffle partitions for small data\n",
    "SET spark.sql.shuffle.partitions = 8;  -- Default is 200, way too much for small tables\n",
    "\n",
    "-- Disable cost-based optimizer for small queries\n",
    "SET spark.sql.cbo.enabled = false;\n",
    "\n",
    "-- Disable unnecessary optimizations for small writes\n",
    "SET spark.databricks.delta.optimizeWrite.enabled = false;\n",
    "SET spark.databricks.delta.autoCompact.enabled = false;\n",
    "\n",
    "-- Speed up small file operations\n",
    "SET spark.sql.files.maxPartitionBytes = 134217728;  -- 128MB instead of default 1GB\n",
    "SET spark.sql.files.openCostInBytes = 4194304;      -- 4MB\n",
    "\n",
    "-- Reduce broadcast join threshold for small tables\n",
    "SET spark.sql.autoBroadcastJoinThreshold = 10485760; -- 10MB\n",
    "\n",
    "-- Disable statistics collection for small tables\n",
    "SET spark.sql.statistics.histogram.enabled = false;\n",
    "SET spark.sql.statistics.fallBackToHdfs = false;\n",
    "\n",
    "-------------------------------------\n",
    "SET spark.sql.shuffle.partitions = 8;  -- This often works even in serverless\n",
    "SET spark.sql.files.maxPartitionBytes = 67108864;\n",
    "SET spark.databricks.optimizer.dynamicFilePruning = false;\n",
    "\n",
    "--------------------------------------------------------------\n",
    "-- Set properties on your target tables\n",
    "ALTER TABLE gap_catalog.ads_owner.EVENT_TYPES SET TBLPROPERTIES (\n",
    "    'delta.autoOptimize.optimizeWrite' = 'false',\n",
    "    'delta.autoOptimize.autoCompact' = 'false',\n",
    "    'delta.tuneFileSizesForRewrites' = 'false'\n",
    ");\n",
    "\n",
    "---------------------------------------------------------------\n",
    "-- Use query hints instead of session settings\n",
    "SELECT /*+ REPARTITION(1) */ * FROM small_table;\n",
    "\n",
    "-- For joins, use broadcast hints\n",
    "\n",
    "SELECT /*+ BROADCAST(small_table) */ \n",
    "    a.*, b.* \n",
    "FROM large_table a \n",
    "JOIN small_table b ON a.id = b.id;\n",
    "\n",
    "-- Disable AQE at query level using hints\n",
    "SELECT /*+ COALESCE(1) */ * FROM small_table;\n",
    "\n",
    "------------------------------------------------------------\n",
    "EXPLAIN EXTENDED \n",
    "--select /*+ COALESCE(1) */ * from gap_catalog.ads_owner.case_object_status;\n",
    "insert  into gap_catalog.ads_owner.EVENT_TYPES \n",
    " ( EVETP_KEY, \n",
    "   EVETP_SOURCE_ID, \n",
    "   EVETP_SOURCE_SYSTEM_ID, \n",
    "   EVETP_SOURCE_SYS_ORIGIN, \n",
    "   EVETP_DESC, \n",
    "   EVETP_EVENT_TABLE_NAME, \n",
    "   EVETP_VALID_FROM, \n",
    "   EVETP_VALID_TO, \n",
    "   EVETP_CURRENT_FLAG, \n",
    "   EVETP_DELETED_FLAG, \n",
    "   EVETP_INSERTED_DATETIME, \n",
    "   EVETP_INSERT_PROCESS_KEY, \n",
    "   EVETP_UPDATED_DATETIME, \n",
    "   EVETP_UPDATE_PROCESS_KEY, \n",
    "   EVETP_TARGET, \n",
    "   EVETP_DATA_PATH)\n",
    "select /* + COALESCE(1) */ EVETP_KEY_NEW  AS EVETP_KEY,  -- Add the offset to maintain key sequence, \n",
    "   EVETP_SOURCE_ID, \n",
    "   EVETP_SOURCE_SYSTEM_ID, \n",
    "   EVETP_SOURCE_SYS_ORIGIN, \n",
    "   EVETP_DESC, \n",
    "   EVETP_EVENT_TABLE_NAME, \n",
    "   to_date(CAST('2025-08-14' AS DATE),'yyyy-MM-dd') as EVETP_VALID_FROM, \n",
    "   DATE('3000-01-01') as EVETP_VALID_TO, \n",
    "   'Y' as EVETP_CURRENT_FLAG, \n",
    "   tech_del_flg as EVETP_DELETED_FLAG, \n",
    "   CURRENT_TIMESTAMP() as EVETP_INSERTED_DATETIME, \n",
    "   CAST(-999 AS BIGINT) as EVETP_INSERT_PROCESS_KEY, \n",
    "   CURRENT_TIMESTAMP() as EVETP_UPDATED_DATETIME, \n",
    "   CAST(-999 AS BIGINT) as EVETP_UPDATE_PROCESS_KEY, \n",
    "   EVETP_TARGET, \n",
    "   EVETP_DATA_PATH\n",
    "  from gap_catalog.ads_etl_owner.DIFF_ADS_RDS_EVENT_TYPES_REGISTRATIONACTION_ADS_MAP_SCD_DIFF\n",
    " where tech_new_rec = 'Y';\n",
    "\n",
    " --------------------------------------------------------------------------------------\n",
    " -- Tag the session\n",
    "SET spark.sql.execution.id = 'daily_etl_job_001';\n",
    "SET custom.job.name = 'customer_data_refresh';\n",
    "SET custom.execution.timestamp = current_timestamp();\n",
    "\n",
    "-- Your queries here\n",
    "DECLARE execution_id STRING DEFAULT 'ADS_RDS1';\n",
    "\n",
    "SELECT /*'${execution_id}'*/\n",
    "* FROM \n",
    "gap_catalog.ads_owner.case_status;\n",
    "\n",
    "-- Query the system tables (available in Unity Catalog)\n",
    "SELECT \n",
    "    statement_id,\n",
    "    statement_text,\n",
    "    start_time,\n",
    "    end_time,\n",
    "    TIMESTAMPDIFF(SECOND, start_time, end_time) as runtime_seconds,\n",
    "    total_task_duration_ms,\n",
    "    execution_status,\n",
    "    compute,\n",
    "executed_by,\n",
    "    executed_by_user_id,\n",
    "    statement_text,\n",
    "    statement_type,\n",
    "    total_duration_ms,\n",
    "waiting_for_compute_duration_ms,\n",
    "waiting_at_capacity_duration_ms,\n",
    "    execution_duration_ms,\n",
    "    compilation_duration_ms,\n",
    "    total_task_duration_ms,\n",
    "    result_fetch_duration_ms\n",
    "    read_partitions,\n",
    "    produced_rows,\n",
    "    from_result_cache,\n",
    "    CAST(statement_text AS varchar(100))\n",
    "FROM system.query.history \n",
    "WHERE start_time >= current_date() - INTERVAL 1 DAY\n",
    "   --AND statement_text LIKE '%ADS_RDS1%'\n",
    "--ORDER BY start_time DESC\n",
    "--AND statement_text <> ''\n",
    ";\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4,
    "sqlQueryOptions": {
     "applyAutoLimit": true,
     "catalog": "cis_catalog",
     "schema": "default"
    }
   },
   "notebookName": "SQL temp.dbquery.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
